{
  "query": "machine learning AND 'open access'",
  "pipeline_summary": {
    "total_papers": 20,
    "with_abstracts": 20,
    "with_pmcids": 7,
    "with_fulltext_potential": 7,
    "downloaded_pdfs": 6,
    "extracted_text": 0,
    "mode": "full_paper",
    "success_rates": {
      "abstract_coverage": 100.0,
      "pmcid_conversion": 35.0,
      "pdf_download": 85.71428571428571,
      "text_extraction": 0.0
    }
  },
  "papers": [
    {
      "pmid": "PMID:41004655",
      "pmcid": null,
      "title": "Electroencephalography microstates as biomarkers for screening Alzheimer's disease: Feasibility analysis and a machine learning classification scheme.",
      "journal": "Journal of Alzheimer's disease : JAD",
      "abstract": "BackgroundElectroencephalography (EEG) microstate analysis has emerged as a key methodology for elucidating the brain's dynamic repertoire, providing a pivotal neurophysiological framework for the identification of cognitive impairment.ObjectiveThis study was aimed to analyze the EEG microstates in Alzheimer's disease (AD) based on a publicly accessible EEG dataset and additionally using support vector machine models to separate the healthy controls and AD patients.MethodsThis scalp EEG dataset from an open-source included 36 AD patients and 29 healthy controls. All EEG data underwent standardized preprocessing incorporating a 0.5-35 Hz band-pass filter and automated artifact rejection. The EEG data were subsequently partitioned into 20-s segments for microstate analysis, generating temporally aligned sequences characterized by canonical four-class spatial configurations.ResultsA total of 24 features were extracted from microstate sequences, including coverage, mean duration, occurrence, and transition probabilities between each two microstates. The statistical testing results indicated that there were significant differences in 21 features between AD patients and healthy controls. Based on the features of statistical significance, we implemented support vector machine models to distinguish the AD patients from the healthy controls, achieving an averaged classification accuracy of 75.8% in a 5-fold cross-subject validation via 10 times repeated random trials.ConclusionsThe EEG microstate analysis methods was a non-invasive, convenient, and efficient technical pathway and could be adopted for identifying AD.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:41002654",
      "pmcid": null,
      "title": "A data-driven approach to interfacial polymerization exploiting machine learning for predicting thin-film composite membrane formation.",
      "journal": "Materials horizons",
      "abstract": "Polymeric thin-film membranes prepared by interfacial polymerization are the cornerstone of liquid separation, with the potential to reduce industrial waste and energy consumption. However, the limited diversity of monomers may hinder further development by restricting the accessible chemical space. To address this, we propose a divide & conquer approach for the interfacial polymerization membrane development pipeline. We constructed a dataset using 18 organic- and 73 water-phase monomers, conducting 1246 interfacial reactions and analyzing membranes",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:41001881",
      "pmcid": null,
      "title": "MITE: the Minimum Information about a Tailoring Enzyme database for capturing specialized metabolite biosynthesis.",
      "journal": "Nucleic acids research",
      "abstract": "Secondary or specialized metabolites show extraordinary structural diversity and potent biological activities relevant for clinical and industrial applications. The biosynthesis of these metabolites usually starts with the assembly of a core 'scaffold', which is subsequently modified by tailoring enzymes to define the molecule's final structure and, in turn, its biological activity profile. Knowledge about reaction and substrate specificity of tailoring enzymes is essential for understanding and computationally predicting metabolite biosynthesis, but this information is usually scattered in the literature. Here, we present MITE, the Minimum Information about a Tailoring Enzyme database. MITE employs a comprehensive set of parameters to annotate tailoring enzymes, defining substrate and reaction specificity by the expressive reaction SMARTS (Simplified Molecular Input Line Entry System Arbitrary Target Specification) chemical pattern language. Both human and machine readable, MITE can be used as a knowledge base, for in silico biosynthesis, or to train machine-learning applications, and tightly integrates with existing resources. Designed as a community-driven and open resource, MITE employs a rolling release model of data curation and expert review. MITE is freely accessible at https://mite.bioinformatics.nl/.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40998935",
      "pmcid": null,
      "title": "Non-invasive detection of choroidal melanoma via tear-derived protein corona on gold nanoparticles: a machine learning approach.",
      "journal": "Scientific reports",
      "abstract": "This study investigates the feasibility of using tear sample analysis, based on protein corona formation on gold nanoparticles combined with electrospray ionization mass spectrometry (ESI-MS) and machine learning techniques, as a non-invasive approach for the detection of choroidal melanoma. The aim is to assess whether protein-nanoparticle interactions can support early and reliable identification of this ocular condition. Tear samples were collected using Schirmer strips from six healthy individuals and six patients diagnosed with choroidal melanoma, with subsequent augmentation to 18 samples per group. Gold nanoparticles (AuNPs, ~ 20 nm) were synthesized via citrate reduction and incubated with tear samples to form protein coronas, which were analyzed using ESI-MS. Eight statistical and entropy-based features (mean, variance, skewness, kurtosis, Shannon entropy, approximate entropy, sample entropy, and permutation entropy) were extracted from spectral data. Additionally, Continuous Wavelet Transform (CWT) with Mexican hat wavelet was applied to convert mass spectrometry data into 128 \u00d7 128 RGB images for deep learning analysis. Classification was performed using traditional machine learning models (Random Forest, Support Vector Machine, Decision Tree, Deep Neural Network) and transfer learning with pre-trained CNNs (VGG16, ResNet50, Xception), evaluated through 5-fold cross-validation. Significant differences in spectral intensity parameters were observed between healthy individuals and choroidal melanoma patients (p < 0.001), with notably lower Mean_Intensity values in cancer patients (56.41 \u00b1 46.06 vs. 111.02 \u00b1 10.01, Cohen's d = 1.64). While m/z parameters showed moderate differences that didn't reach statistical significance (p = 0.082), entropy-based features demonstrated strong discriminative power. Among traditional machine learning models, Random Forest achieved the highest accuracy (0.959 \u00b1 0.003) and ROC AUC (0.993 \u00b1 0.000) with remarkable computational efficiency (3.90 s per fold). For deep learning approaches using CWT-generated images, VGG16 demonstrated superior performance (Accuracy: 0.976 \u00b1 0.008, ROC AUC: 0.997 \u00b1 0.002) despite requiring significantly higher computational resources (1349.52 s per fold). This study demonstrates that tear sample analysis using protein corona formation on gold nanoparticles with ESI-MS and advanced machine learning techniques offers a promising non-invasive approach for choroidal melanoma detection with performance metrics that compare favorably to existing methods. The significant differences in spectral intensity parameters between groups suggest distinctive proteomic signatures that can be leveraged for diagnostic purposes. While both traditional machine learning and deep learning approaches achieved exceptional performance, each offers distinct advantages in terms of computational efficiency and feature extraction capabilities.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40998314",
      "pmcid": null,
      "title": "PYSEQM 2.0: Accelerated Semiempirical Excited-State Calculations on Graphical Processing Units.",
      "journal": "Journal of chemical theory and computation",
      "abstract": "We report the development and implementation of electronic excited-state capabilities for semiempirical quantum chemical methods at both the Configuration Interaction Singles and Time-Dependent Hartree-Fock levels of theory, integrated within the PYSEQM 2.0 software package (https://github.com/lanl/PYSEQM). PYSEQM is a Python-based package designed for efficient and scalable quantum chemical simulations. Leveraging the PyTorch framework enables PYSEQM to benefit from automatic differentiation and GPU acceleration, leading to substantial performance gains in molecular property evaluations. In particular, our implementation enables efficient calculation of excited-state properties for large molecular systems. Benchmarking on systems with up to a thousand atoms demonstrates that excited-state computations can be completed in under a minute on modern GPUs, making this approach particularly suitable for high-throughput screening, real-time feedback in interactive simulations, and large-scale dynamical studies. Additionally, PYSEQM includes a machine learning interface that supports Hamiltonian parameter reoptimization and neural network training. These capabilities open new avenues for data-driven excited-state dynamics simulations, offering a path toward combining quantum chemical rigor with machine learning efficiency. Overall, this work facilitates access to excited-state quantum chemistry for large systems, while laying the foundation for future hybrid quantum-machine-learning approaches in photochemistry, photophysics, and materials discovery.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40997534",
      "pmcid": null,
      "title": "An open-access computational fingerprinting workflow for source classifications of neat gasoline using GC \u00d7 GC-TOFMS and Machine Learning.",
      "journal": "Journal of chromatography. A",
      "abstract": "Advances in sensitivity and selectivity of multidimensional chromatography have enhanced our ability to better characterize and identify sources of neat gasoline used in arson cases. However, the large and complex chemical datasets generated present a significant challenge for data management and interpretation, requiring robust computational analysis techniques. In this study, we present a novel, open-access computational fingerprinting workflow to develop regional database of gasoline profiles and source tracking of gasoline samples from local gas stations for arson investigations. The computational workflow included data reduction, normalization, clustering analyses, feature selection and supervised machine learning (ML) to explore the differentiation between gasoline sources. Chromatographic features (n = 25,415) from multidimensional gas chromatography-time of flight mass spectrometry (GC \u00d7 GC-TOFMS) analysis of 69 neat gasoline samples, collected from 10 gas stations in Alberta (Canada), were used in supervised ML for the classification of neat gasoline samples. Fifty chemical features selected using recursive feature addition (RFA), with associated chemistries of n-alkanes, alkenes, cycloalkanes, and aromatics, were found to differentiate local gas stations. Despite overlapping between gas stations in clustering analyses, an average improvement of 18 % in ML accuracy was achieved by using decision tree-based ML classifiers coupled with RFA as compared to using all features. Our open-source computational workflow ensures transparency and reproducibility in creating a method and regional database for the distinction of gasoline sources commonly used in wildfire arson. The workflow enables forensic analysts to integrate additional chemical features into existing target chemical libraries within the ASTM E1618-19 protocol, enhancing ignitable liquid identification without requiring extensive re-training of computational models or programming expertise.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40996705",
      "pmcid": "PMC12462375",
      "title": "The impact of curation errors in the PDBBind Database on machine learning predictions of protein-protein binding affinity.",
      "journal": "Database : the journal of biological databases and curation",
      "abstract": "The PDBBind database has been widely utilized for the computational prediction of protein-protein binding affinities. While the accuracy of the PDBBind-curated equilibrium dissociation constants (KD) has been reported for the protein-ligand subset of the PDBBind database, the curation accuracy has not been reported for the protein-protein subset. Here, we present a detailed manual analysis for the subset of PDBBind records with PubMed Central Open Access primary publications and find that ~19% of these records had KD values that were not supported by their primary publications. The impact of these putative curation errors on the machine learning-based prediction of KD from experimental protein-protein 3D structures was evaluated and correcting the curation errors improved the Pearson correlation coefficient between measured and random forest-predicted log10(KD) values by ~8 percentage points. This finding underscores the importance of dataset accuracy for computational modelling and highlights the need for more stringent curation processes when extracting information from the scientific literature.",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12462375.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40996470",
      "pmcid": null,
      "title": "An open deep learning-based framework and model for tooth instance segmentation in dental CBCT.",
      "journal": "Clinical oral investigations",
      "abstract": "OBJECTIVES: Current dental CBCT segmentation tools often lack accuracy, accessibility, or comprehensive anatomical coverage. To address this, we constructed a densely annotated dental CBCT dataset and developed a deep learning model, OraSeg, for tooth-level instance segmentation, which is then deployed as a one-click tool and made freely accessible for non-commercial use. MATERIALS AND METHODS: We established a standardized annotated dataset covering 35 key oral anatomical structures and employed UNetR as the backbone network, combining Swin Transformer and the spatial Mamba module for multi-scale residual feature fusion. The OralSeg model was designed and optimized for precise instance segmentation of dental CBCT images, and integrated into the 3D Slicer platform, providing a graphical user interface for one-click segmentation. RESULTS: OralSeg had a Dice similarity coefficient of 0.8316 \u00b1 0.0305 on CBCT instance segmentation compared to SwinUNETR and 3D U-Net. The model significantly improves segmentation performance, especially in complex oral anatomical structures, such as apical areas, alveolar bone margins, and mandibular nerve canals. CONCLUSION: The OralSeg model presented in this study provides an effective solution for instance segmentation of dental CBCT images. The tool allows clinical dentists and researchers with no AI background to perform one-click segmentation, and may be applicable in various clinical and research contexts. CLINICAL RELEVANCE: OralSeg can offer researchers and clinicians a user-friendly tool for tooth-level instance segmentation, which may assist in clinical diagnosis, educational training, and research, and contribute to the broader adoption of digital dentistry in precision medicine.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40993509",
      "pmcid": "PMC12462135",
      "title": "Glucostats: an efficient Python library for glucose time series feature extraction and visual analysis.",
      "journal": "BMC bioinformatics",
      "abstract": "BACKGROUND: The advancement of technology and continuous glucose monitoring (CGM) systems has introduced several computational and technical challenges for clinicians and researchers. The growing volume of CGM data necessitates the development of efficient computational tools capable of handling and processing this information effectively. This paper introduces GlucoStats, an open-source and multi-processing Python library designed for efficient computation and visualization of a comprehensive set of glucose metrics derived from CGM. It simplifies the traditionally time-consuming and error-prone process of manual CGM metrics calculation, making it a valuable tool for both clinical and research applications. RESULTS: Its modular design ensures easy integration into predefined workflows, while its user-friendly interface and extensive documentation make it accessible to a broad audience, including clinicians and researchers. GlucoStats offers several key features: (i) window-based time series analysis, enabling time series division into smaller 'windows' for detailed temporal analysis, particularly beneficial for CGM data; (ii) advanced visualization tools, providing intuitive, high-quality visualizations that facilitate pattern recognition, trend analysis, and anomaly detection in CGM data; (iii) parallelization, leveraging parallel computing to efficiently handle large CGM datasets by distributing computations across multiple processors; and (iv) scikit-learn compatibility, adhering to the standardized interface of scikit-learn to allow an easy integration into machine learning pipelines for end-to-end analysis. CONCLUSIONS: GlucoStats demonstrates high efficiency in processing large-scale medical datasets in minimal time. Its modular design enables easy customization and extension, making it adaptable to diverse research and clinical needs. By offering precise CGM data analysis and user-friendly visualization tools, it serves both technical researchers and non-technical users, such as physicians and patients, with practical and research-driven applications.",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12462135.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40991191",
      "pmcid": null,
      "title": "Detection and classification of medical images using deep learning for chronic kidney disease.",
      "journal": "International urology and nephrology",
      "abstract": "Chronic kidney disease (CKD) is an advancing disease which significantly impacts global healthcare, requiring early detection and prompt treatment is required to prevent its advancement to end-stage renal disease. Conventional diagnostic methods tend to be invasive, lengthy, and costly, creating a demand for automated, precise, and efficient solutions. This study proposes a novel technique for identifying and classifying CKD from medical images by utilizing a Convolutional Neural Network based Crow Search (CNN based CS) algorithm. The method employs sophisticated pre-processing techniques, including Z-score standardization, min-max normalization and robust scaling to improve the input data's quality. Selection of features is carried out using the chi-square test, and the Crow Search Algorithm (CSA) further optimizes the feature set for the improvement of accuracy classification and effectivess. The CNN architecture is employed to capture complex patterns using deep learning methods to accurately classify CKD in medical pictures. The model optimized and examined using an open access Kidney CT Scan data set. It achieved 99.05% accuracy, 99.03% Area under the Receiver Operating Characteristic Curve (AUC-ROC), and 99.01% Area under the precision-recall curve (PR-AUC), along with high precision (99.04%), recall (99.02%), and F1-score (99.00%). The results show that the CNN-based CS method delivers high accuracy and improved diagnostic precision related to conventional machine learning techniques. By incorporating CSA for feature optimization, the approach minimizes redundancy and improves model interpretability. This makes it a promising tool for automated CKD diagnosis, contributing to the development of AI-driven medical diagnostics and providing a scalable solution for early detection and management of CKD.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40989895",
      "pmcid": "PMC12453057",
      "title": "BattyCoda: A novel open-source software for bat call annotation and classification.",
      "journal": "Ecological informatics",
      "abstract": "The field of acoustic communication needs tools that facilitate the annotation and labeling of animal calls. Bat acoustic libraries gathered over the past few decades have primarily focused on compiling echolocation calls, which have been leveraged to develop machine learning algorithms capable of classifying bat species. However, because these classification methods require large training datasets, they have not yet been generalized to classify types of bat communication calls. Communication call repertoires in bats are wide, and distinct syllables occur with varying frequency, with some call types being recorded only rarely. Furthermore, collecting Classification communication calls poses greater technical challenges, making these calls more difficult to capture reliably. Here, we present BattyCoda, an open-access, customizable tool to categorize and label bat communication call types within the repertoire of a species using small training datasets (tens to hundreds of labeled calls). In this work, we compiled an initial training dataset of 11 types of big brown bat (",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12453057.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40986859",
      "pmcid": "PMC12456872",
      "title": "Understanding Cancer Survivorship Care Needs Using Amazon Reviews: Content Analysis, Algorithm Development, and Validation Study.",
      "journal": "JMIR cancer",
      "abstract": "BACKGROUND: Complementary therapies are being increasingly used by cancer survivors. As a channel for customers to share their feelings, outcomes, and perceived knowledge about the products purchased from e-commerce platforms, Amazon consumer reviews are a valuable real-world data source for understanding cancer survivorship care needs. OBJECTIVE: In this study, we aimed to highlight the potential of using Amazon consumer reviews as a novel source for identifying cancer survivorship care needs, particularly related to symptom self-management. Specifically, we present a publicly available, manually annotated corpus derived from Amazon reviews of health-related products and develop baseline natural language processing models using deep learning and large language model (LLM) to demonstrate the usability of this dataset. METHODS: We preprocessed the Amazon review dataset to identify sentences with cancer mentions through a rule-based method and conducted content analysis including text feature analysis, sentiment analysis, topic modeling, cancer type, and symptom association analysis. We then designed an annotation guideline, targeting survivorship-relevant constructs. A total of 159 reviews were annotated, and baseline models were developed based on deep learning and large language model (LLM) for named entity recognition and text classification tasks. RESULTS: A total of 4703 sentences containing positive cancer mentions were identified, drawn from 3349 reviews associated with 2589 distinct products. The identified topics through topic modeling revealed meaningful insights into cancer symptom management and survivorship experiences. Examples included discussions of green tea use during chemotherapy, cancer prevention strategies, and product recommendations for breast cancer. Top 15 symptoms in reviews were also identified, with pain being the most frequent symptom, followed by inflammation, fatigue, etc. The annotation labels were designed to capture cancer types, indicated symptoms, and symptom management outcomes. The resulting annotation corpus contains 2067 labels from 159 Amazon reviews. It is publicly accessible, together with the annotation guideline through the Open Health Natural Language Processing (OHNLP) GitHub. Our baseline model, Bert-base-cased, achieved the highest weighted average F1-score, that is, 66.92%, for named entity recognition, and LLM gpt4-1106-preview-chat achieved the highest F1-score for text classification tasks, that is, 66.67% for \"Harmful outcome,\" 88.46% for \"Favorable outcome\" and 73.33% for \"Ambiguous outcome.\" CONCLUSIONS: Our results demonstrate the potential of Amazon consumer reviews as a novel data source for identifying persistent symptoms, concerns, and self-management strategies among cancer survivors. This corpus, along with the baseline natural language processing models developed for named entity recognition and text classification, lays the groundwork for future methodological advancements in cancer survivorship research. Importantly, insights from this study could be evaluated against established clinical guidelines for symptom management in cancer survivorship care. By revealing the feasibility of using consumer-generated data for mining survivorship-related experiences, this study offers a promising foundation for future research and argumentation analysis aimed at improving long-term outcomes and support for cancer survivors.",
      "has_fulltext": true,
      "download_success": false,
      "pdf_path": null,
      "error_message": "All download strategies failed"
    },
    {
      "pmid": "PMID:40982485",
      "pmcid": "PMC12453211",
      "title": "A machine learning framework for estimating the probability of blacklegged tick population establishment in eastern Canada using Earth observation data.",
      "journal": "PloS one",
      "abstract": "Ixodes scapularis ticks are the primary vector of Lyme disease (LD) in North America, and their range has expanded into southeastern and southcentral Canada with climate change. This study presents a comprehensive machine learning (ML) framework to estimate the probability of blacklegged tick population establishment as measured using active tick surveillance data. Environmental predictor variables were derived from Earth observation (EO) data at multiple spatial scales to assess their individual contributions in the prediction models. Among the tested ML algorithms, XGBoost emerged as the top-performing model, achieving high sensitivity (0.83) and specificity (0.71) in predicting population establishment. Performance was optimized when using predictor variables derived from a 1 km radius around surveillance sites. Top predictors included cumulative annual degree-days above 0\u00b0C and maximum temperature of warmest month, reflecting the importance of temperature in enabling tick survival and reproduction. Additional predictor variables of high importance included silty soil (lower clay content) with slightly higher than average SOC and pH, and land cover types that contained broadleaf forests (percent mixed forest, percent broadleaf) and less urban areas. By integrating ML with open access EO data, this study demonstrates that accurate, easily updatable risk maps can be produced to support public health management of LD, and more broadly, the growing threat of tick-borne diseases in a changing climate.",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12453211.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40978894",
      "pmcid": "PMC12446892",
      "title": "Machine Learning Approach to Predict Emergency Cesarean Sections Among Nulliparous Women.",
      "journal": "Cureus",
      "abstract": "Introduction The obstetrical team's efforts are consistently focused on minimizing the number of cesarean sections, particularly in nulliparous women. One of the most crucial steps is to understand the risk factors that predispose the woman to a cesarean section. This study aimed to identify the predictors of emergency cesarean sections in nulliparous women using a machine learning approach. Methods A retrospective cohort study was carried out at a maternal tertiary center in Iran among nulliparous women with a single cephalic pregnancy, \u226537 weeks of gestation, and induced or spontaneous labor, who gave birth between January 2020 and December 2022. The exclusion criteria were maternal request for cesarean section or those who delivered via cesarean section before the onset of labor. The rate of emergency cesarean section and the performance of machine learning in predicting emergency cesarean section were the outcome measures. Twenty-three factors potentially linked to the method of childbirth were initially identified, and included age, educational level, place of residence, medical insurance, nationality, attending prenatal education course, gestational age, the onset of labor, having a doula during the labor process, analgesia during labor, history of infertility, history of abortion, maternal anemia, cardiovascular disease, diabetes, maternal obesity, preeclampsia, prolonged rupture of membrane, placenta abruption, meconium amniotic fluid, intrauterine growth retardation, newborn weight, and newborn sex. The input data were fed into seven machine learning models: linear regression, logistic regression, decision tree classification, random forest classification, XGBoost classification, permutation classification (KNN), and deep learning. Results During the study period, 1916 (71.8%) of the 2668 births were vaginal, while 752 (28.2%) were by cesarean section. Cesarean sections were more common in mothers of advanced age and with a higher level of education. Attending a prenatal education course was also linked to the method of childbirth. Induced labor was more common in women who had a cesarean section. Those who had a doula were more likely to give birth vaginally. Maternal diabetes, obesity, preeclampsia, thyroid disease, placental abruption, meconium amniotic fluid, and fetal macrosomia were all linked to the method of childbirth. The area under the curve (AUC) for each model turned out to be: linear regression (0.86), XGBoost classification (0.83), logistic regression (0.79), deep learning (0.78), permutation classification (K-Nearest Neighbors or KNN) (0.77), decision tree classification (0.76), and random forest classification (0.72). Linear regression had a better diagnostic performance than other models with the area under the ROC curve (AUROC): 0.86, accuracy: 0.82, precision: 0.79, recall: 0.85, and F1-Score: 0.79). The linear regression model showed that advanced maternal age, advanced maternal education, diabetes, preeclampsia, placenta abruption, hypothyroidism, meconium amniotic fluid, late-term pregnancy, doula support, and attending prenatal courses were predictors of emergency cesarean section in nulliparous women. Conclusions Utilizing a clinical database and various machine learning algorithms showed potential in predicting emergency cesarean section. Additional prospective research, including intrapartum clinical characteristics, is essential for improving the accuracy of prediction accuracy.",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12446892.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40977760",
      "pmcid": "PMC12447317",
      "title": "The online survey in qualitative research: can AI act as a probing tool?",
      "journal": "Frontiers in research metrics and analytics",
      "abstract": "Surveys are commonly associated with quantitative methods, yet there is growing recognition of their potential to yield qualitative insights into complex social phenomena. However, the effectiveness of open-ended survey questions is often limited by issues such as respondent fatigue and low-quality responses. To address these limitations, researchers are increasingly exploring the use of artificial intelligence (AI) to support dynamic survey design, probing questions, and participant engagement. This article explores the role of qualitative surveys in social science research, by considering their alignment with qualitative paradigms. The content assesses how AI-powered features, such as machine learning and chatbot-driven interfaces, can enhance data collection through adaptive questioning. The article also discusses key challenges related to data quality, participant inclusivity, and ethical considerations. Particular attention is given to the concept of \"felt anonymity\" in online surveys, which can encourage candid disclosures on sensitive topics and broaden participation across diverse populations. When designed with ethical and methodological care, qualitative surveys can thus serve as powerful tools for accessing underrepresented perspectives. By integrating AI into qualitative survey design, researchers can enhance both the richness and reach of their data. This article argues that AI-powered qualitative surveys, especially those capable of dynamic probing, offer a promising hybrid approach, bridging the scalability of surveys with the responsiveness of interviews, and calls for further empirical study of their ethical and epistemological implications.",
      "has_fulltext": true,
      "download_success": true,
      "pdf_path": "pdfs/12447317.pdf",
      "error_message": null
    },
    {
      "pmid": "PMID:40973394",
      "pmcid": null,
      "title": "Revolutionizing structural biology: AI-driven protein structure prediction from AlphaFold to next-generation innovations.",
      "journal": "Advances in protein chemistry and structural biology",
      "abstract": "Protein structure modeling from the prediction algorithm has become a valuable tool in biology and medicine with computational advances. Accurate protein structure prediction is critical in druglike compound discovery, disease mechanism understanding, and protein engineering because it provides molecular level insights into protein folding and its effects on molecular and cellular function. This chapter covers the evolution of protein structure prediction, from traditional methods like homology modeling, threading, and ab initio procedures and the new emerging AlphaFold's influence. AlphaFold's highly recognized precision level and open-access data democratized structural biology research, and that lead to inspiring new prediction models like RoseTTAFold and OmegaFold tools. Alpha Folds design, methodology, and highly accurate performance are thoroughly examined, and comparisons are performed with similar tools. We also highlight limitations, such as protein complex and dynamics forecasting, post-AlphaFold developments in structural databases, computer resources, and multi-scale modeling. Protein structure modeling and predictions have a wide range of applications in biomedical research, including drug discovery, functional annotation, and synthetic biology. Future directions include the integration of protein structure prediction with systems biology and genomics, as well as the use of next-generation AI and quantum computing to boost prediction accuracy. This research emphasizes AI's importance in structural biology and envisions a future in which predictive tools will provide comprehensive insights into protein function, dynamics, and therapeutic potential.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40971735",
      "pmcid": null,
      "title": "MyESL: A software for evolutionary sparse learning in molecular phylogenetics and genomics.",
      "journal": "Molecular biology and evolution",
      "abstract": "Evolutionary sparse learning (ESL) uses supervised machine learning to build evolutionary models where genomic sites and loci are parameters. It uses the Least Absolute Shrinkage and Selection Operator (LASSO) with bi-level sparsity to connect a specific phylogenetic hypothesis with sequence variation across genomic loci. The MyESL software addresses the need for open-source tools to perform ESL analyses, offering features to pre-process input phylogenomic alignments, post-process output models to generate molecular evolutionary metrics, and make LASSO regression adaptable and efficient for phylogenetic trees and alignments. The core of MyESL, which constructs models with logistic regressions using bi-level sparsity, is written in C++. Its input data pre-processing and result post-processing tools are developed in Python. Compared to other tools, MyESL is more computationally efficient and provides evolution-friendly input and output options. These features have already enabled the use of MyESL in two phylogenomic applications, one to identify outlier sequences and fragile clades in inferred phylogenies and another to build a genetic model of convergent traits. In addition to the use in a Python environment, MyESL is available as a standalone executable compatible across multiple platforms and can be directly integrated into scripts and third-party software. The source code, executable, and documentation for MyESL are openly accessible at https://github.com/kumarlabgit/MyESL.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40971497",
      "pmcid": null,
      "title": "Drug and Clinical Candidate Drug Data in ChEMBL.",
      "journal": "Journal of medicinal chemistry",
      "abstract": "ChEMBL is a large-scale, open-access, FAIR database of bioactive molecules with drug-like properties. ChEMBL 35 contains 17,500 approved drugs, and drugs that are progressing through the clinical development pipeline. Drug curation has formed an integral part of the core offering of the ChEMBL database since its inception. The paper is a reference guide to present the principles of why the ChEMBL drug data has been curated in a particular manner so that data users can better understand the nature of the data. The drug data include information on: names, synonyms and trade names, chemical structure or biological sequence, data sources, indications, mechanisms, warnings and drug properties such as maximum phase of development, type of molecule, prodrug status and first approval. The integrated nature of the drug data within the context of a bioactivity resource enables the wide use of the data set in drug discovery, AI and machine learning.",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40967497",
      "pmcid": null,
      "title": "Toward equitable environmental exposure modeling through convergence of data, open, and citizen sciences: an example of air pollution exposure modeling amidst increasing wildfire smoke.",
      "journal": "Environmental research",
      "abstract": "Exposure modeling is critical in environmental epidemiology and human health but may face challenges (e.g., skewed data, unequal error, context-insensitive validation, and computational demands). Modeling decisions reflect the intended use of the models and the values that modelers prioritize. We aimed to provide a conceptual framework and machine learning (ML) modeling protocols that address these issues. With 500m-gridded hourly PM",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    },
    {
      "pmid": "PMID:40963377",
      "pmcid": null,
      "title": "Thermodynamics of Ring-Opening Polymerisation Informatics Collection (TROPIC): a database to enable polymer chemical recycling.",
      "journal": "Faraday discussions",
      "abstract": "The development of artificial intelligence and machine learning in chemistry is opening new avenues for data-driven discoveries. However, the application of such methodologies in polymer chemistry has been hampered due to the complex structure-properties relationship of polymers and the lack of (meta)data available. Recent efforts have been made to experimentally determine or computationally evaluate thermodynamic parameters associated with (de)polymerisation reactions, such as enthalpy and entropy of polymerisation, as well as ceiling temperature, to design polymers primed for chemical recycling. Here, we report TROPIC (Thermodynamics of Ring-Opening Polymerisation Informatics Collection), an open-source database harnessing experimental and computational thermodynamic parameters for ring-opening polymerisation (ROP) from the academic literature. TROPIC links thermodynamic parameters with the experimental conditions or the computation methodologies used to determine them, to allow further analysis. TROPIC can be accessed",
      "has_fulltext": false,
      "download_success": false,
      "pdf_path": null,
      "error_message": "No applicable download strategies found"
    }
  ],
  "llm_analysis_summary": null,
  "visualization_files": []
}